{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e85e2aaa-4382-4404-bd79-0b5d19a8021b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/workspace/persistent/thesis/OadTR'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "from custom_dataset import METEORDataLayer\n",
    "import transformer_models\n",
    "from custom_utils import generate_dict, ModelConfig, get_multilabel_conf_mat, eval_experiment\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# set constants\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "EXPERIMENT_PATH = 'experiments/30_FPS/init_try'\n",
    "CHECKPOINT = 'DEFAULT'\n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "419b9680",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97c029b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 243 videos\n",
      "position encoding : learned\n",
      "position decoding : learned\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "experiments/30_FPS/init_try:   0%|          | 0/748 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "eval_experiment(EXPERIMENT_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6018bf98",
   "metadata": {},
   "source": [
    "### util functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b95c38",
   "metadata": {},
   "source": [
    "## load config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2428a77-56bf-4bcf-914c-bb6d4dbe2f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_dict = generate_dict(os.path.join(EXPERIMENT_PATH, 'log_dist.txt'))\n",
    "args = ModelConfig(**config_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd2e0db",
   "metadata": {},
   "source": [
    "## load data\n",
    "\n",
    "do this first, since it's most time consuming part and invariant to model config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3669da56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test: 243 videos\n"
     ]
    }
   ],
   "source": [
    "if 'pickle_file_name' not in args.__dict__.keys():\n",
    "    print(args.hidden_dim)\n",
    "    if args.dim_feature == 2048 and '30_FPS' not in EXPERIMENT_CONFIG: # attention based backbone, not 30 FPS\n",
    "        args.pickle_file_name = 'extraction_output_11-02-2023-18-33.pkl'\n",
    "    elif args.dim_feature == 2048 and '30_FPS' in EXPERIMENT_CONFIG: # attention based backbone and 30 FPS\n",
    "        args.pickle_file_name = 'extraction_output_15-02-2023-18-12.pkl'\n",
    "    elif args.dim_feature == 4096: # convolutional backbone\n",
    "        args.pickle_file_name = 'extraction_output_06-01-2023-18-39.pkl'\n",
    "    else:\n",
    "        assert False, \"can't assign pickle file based on easy heuristic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262b302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(args.dataset_file, 'r') as f:\n",
    "        data_info = json.load(f)['METEOR']\n",
    "args.test_session_set = data_info['test_session_set']\n",
    "dataset_test = METEORDataLayer(phase='test', args=args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "72b1e935-1591-4da7-9c45-54d60b1bf8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler_test = torch.utils.data.SequentialSampler(dataset_test)\n",
    "data_loader_test = DataLoader(dataset_test, 512, drop_last=False, pin_memory=True, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0af795ac",
   "metadata": {},
   "source": [
    "## load and prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac79291e-4302-42a3-a569-adfda272bace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position encoding : learned\n",
      "position decoding : learned\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformer_models.VisionTransformer_v3(args=args, img_dim=args.enc_layers,\n",
    "                                             patch_dim=args.patch_dim,\n",
    "                                             out_dim=args.numclass,\n",
    "                                             embedding_dim=args.embedding_dim,\n",
    "                                             num_heads=args.num_heads,\n",
    "                                             num_layers=args.num_layers,\n",
    "                                             hidden_dim=args.hidden_dim,\n",
    "                                             dropout_rate=args.dropout_rate,\n",
    "                                             attn_dropout_rate=args.attn_dropout_rate,\n",
    "                                             num_channels=args.dim_feature,\n",
    "                                             positional_encoding_type=args.positional_encoding_type,\n",
    "                                             with_motion=args.use_flow\n",
    "                                                )\n",
    "model.eval()\n",
    "model.to(DEVICE)\n",
    "\n",
    "\n",
    "# preapare string describing checkpoint\n",
    "if CHECKPOINT.lower() == 'default':\n",
    "    check_str = 'checkpoint.pth'\n",
    "else:\n",
    "    check_str = 'checkpoint{0:04}.pth'.format(CHECKPOINT)\n",
    "    assert check_str in os.listdir(EXPERIMENT_PATH), f\"no checkpoint nr {CHECKPOINT} in {EXPERIMENT_PATH}\"\n",
    "    \n",
    "# load state dict for model and \n",
    "\n",
    "checkpoint = torch.load(os.path.join(EXPERIMENT_PATH, check_str), map_location='cpu')\n",
    "model.load_state_dict(checkpoint['model'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58ba110",
   "metadata": {},
   "source": [
    "## make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4db30d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/748 [00:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 31.75 GiB total capacity; 617.30 MiB already allocated; 13.94 MiB free; 628.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m motion_inputs \u001b[38;5;241m=\u001b[39m motion_inputs\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# generate predictions\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcamera_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmotion_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m out \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# out[1] is decoder target\u001b[39;00m\n\u001b[1;32m     12\u001b[0m out\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/envs/thesis/lib/python3.9/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/workspace/persistent/thesis/OadTR/transformer_models/ViT.py:166\u001b[0m, in \u001b[0;36mVisionTransformer_v3.forward\u001b[0;34m(self, sequence_input_rgb, sequence_input_flow)\u001b[0m\n\u001b[1;32m    164\u001b[0m cls_tokens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcls_token\u001b[39m.\u001b[39mexpand(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    165\u001b[0m \u001b[39m# x = torch.cat((cls_tokens, x), dim=1)\u001b[39;00m\n\u001b[0;32m--> 166\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat((x, cls_tokens), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m# [128, 65, 1024]\u001b[39;00m\n\u001b[1;32m    167\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mposition_encoding(x) \u001b[39m# [128, 65, 1024]\u001b[39;00m\n\u001b[1;32m    168\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpe_dropout(x)   \u001b[39m# not delete\u001b[39;00m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 66.00 MiB (GPU 0; 31.75 GiB total capacity; 617.30 MiB already allocated; 13.94 MiB free; 628.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "y_pred = []\n",
    "y_true = []\n",
    "for batch in tqdm(data_loader_test):\n",
    "    camera_inputs, motion_inputs, enc_target, distance_target, class_h_target, dec_target = batch\n",
    "    # move everything to device\n",
    "    camera_inputs = camera_inputs.to(DEVICE)\n",
    "    motion_inputs = motion_inputs.to(DEVICE)\n",
    "\n",
    "    # generate predictions\n",
    "    out = model(camera_inputs, motion_inputs)\n",
    "    out = out[0] # out[1] is decoder target\n",
    "    out.to('cpu')\n",
    "    y_true.append(class_h_target)\n",
    "    y_pred.append(out)\n",
    "\n",
    "y_true = torch.cat(y_true, dim=0)\n",
    "y_pred = torch.cat(y_pred, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0812d66a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1213"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../../../pvc-meteor/features/extraction_output_06-01-2023-18-39.pkl', 'rb') as f:\n",
    "    file = pickle.load(f)\n",
    "\n",
    "len(file['features'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b32d2134",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "\n",
    "with open('../../../pvc-meteor/features/METEOR_info.json', 'r') as f:\n",
    "    json_file = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2e38e69f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2150., 18016., 21162.,   976.,   432.,     0.,     0.])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution test_anno\n",
    "global_anno = np.zeros(len(json_file['METEOR']['class_index']))\n",
    "for key in json_file['METEOR']['test_session_set']:\n",
    "    cur_anno = file['annotations'][key]['anno'].sum(axis=0)\n",
    "    global_anno = global_anno + cur_anno\n",
    "    \n",
    "global_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "313a17ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9.5940e+03, 8.0405e+04, 8.9837e+04, 2.0470e+03, 1.8330e+03,\n",
       "       4.3000e+01, 4.0000e+00])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# distribution train_anno\n",
    "global_anno = np.zeros(len(json_file['METEOR']['class_index']))\n",
    "for key in json_file['METEOR']['train_session_set']:\n",
    "    cur_anno = file['annotations'][key]['anno'].sum(axis=0)\n",
    "    global_anno = global_anno + cur_anno\n",
    "    \n",
    "global_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d8154aa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OverTaking',\n",
       " 'Overspeeding',\n",
       " 'LaneChange',\n",
       " 'TrafficLight',\n",
       " 'WrongLane',\n",
       " 'WrongTurn',\n",
       " 'Cutting']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_file['METEOR']['class_index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b29d1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import multilabel_confusion_matrix\n",
    "import numpy as np\n",
    "y_true = np.array([[1,1,1,1], [1,0,1,0], [1,0,0,0], [0,1,0,0], [0,0,0,0]])\n",
    "y_pred = np.array([[1,1,0,0], [1,0,1,0], [1,0,1,0], [0,0,0,0], [1,1,1,1]])\n",
    "mat = multilabel_confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6ed2f5c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1]\n",
      " [0 3]]\n"
     ]
    }
   ],
   "source": [
    "print(mat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5cc1bce2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1],\n",
       "       [0, 3]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857633b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "21e8d3179073958173d6cf46a62335ced4bd9060b16a08c7dd2ce09d931d915c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
