{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.models.optical_flow import raft_large\n",
    "from torchvision.models.optical_flow import Raft_Large_Weights\n",
    "from torchvision.utils import flow_to_image\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "from torchvision.models import resnet50, ResNet50_Weights, swin_v2_b, Swin_V2_B_Weights\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from zipfile import ZipFile\n",
    "import xmltodict\n",
    "import os\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import gc\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "# import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.contrib.concurrent import process_map\n",
    "from torchvision.io import read_video\n",
    "import torch\n",
    "\n",
    "from multiprocessing import cpu_count, Pool, log_to_stderr, Manager\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extractor\n",
    "this is the prototype for the 'feature_extractor.py' file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor():\n",
    "    def __init__(self, FPS, **kwargs):\n",
    "        assert FPS in [1, 2, 3, 5, 10, 15, 30], 'for now input musst be perfect divisor of 30 ([1, 2, 3, 5, 10, 15, 30])'\n",
    "\n",
    "        # 30 is the framerate of the METEOR Data,\n",
    "        self.frame_interval = int(30/FPS)\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        self.resize = kwargs.get('resize', (448,448))\n",
    "    \n",
    "        self.batch_size = kwargs.get('batch_size', 32)\n",
    "\n",
    "        self.error_file = dict()\n",
    "\n",
    "        if 'flow_model' in kwargs.keys():\n",
    "            warnings.warn('Using a different flow model may require disabling backprop', category=UserWarning, stacklevel=2)\n",
    "            self.flow_model = kwargs['flow_model']\n",
    "            flow_weights = kwargs['flow_weights']\n",
    "            self.transforms = flow_weights.transforms()\n",
    "\n",
    "        else:\n",
    "            self.flow_model = raft_large(\n",
    "                weights=Raft_Large_Weights.DEFAULT, progress=False).to(self.device)\n",
    "            for param in self.flow_model.parameters():\n",
    "                param.requires_grad = False\n",
    "            flow_weights = Raft_Large_Weights.DEFAULT\n",
    "            self.transforms = flow_weights.transforms()\n",
    "\n",
    "        if 'rgb_model' in kwargs.keys():\n",
    "            self.rgb_model = kwargs['rgb_model']\n",
    "            warnings.warn('Using a different rgb model may require disabling backprop', category=UserWarning, stacklevel=2)\n",
    "        else:\n",
    "            model = resnet50(weights=ResNet50_Weights.DEFAULT, progress=False)\n",
    "            for param in model.parameters():\n",
    "                param.requires_grad = False\n",
    "            self.rgb_model = create_feature_extractor(\n",
    "                model, return_nodes={'flatten': 'flatten'}).to(self.device)\n",
    "\n",
    "        if 'vid_dir' in kwargs.keys():\n",
    "            self.vid_dir = kwargs['vid_dir']\n",
    "        else:\n",
    "            self.vid_dir = '/workspace/pvc-meteor/Raw_Videos/'\n",
    "        if 'anno_zip_dir' in kwargs.keys():\n",
    "            self.anno_zip_dir = kwargs['anno_zip_dir']\n",
    "        else:\n",
    "            self.anno_zip_dir = '/workspace/pvc-meteor/downloads/Frame XML Annotations/'\n",
    "        if 'output_dir' in kwargs.keys():\n",
    "            self.output_dir = kwargs['output_dir']\n",
    "        else:\n",
    "            self.output_dir = '/workspace/pvc-meteor/features/'\n",
    "\n",
    "        if 'labels' in kwargs.keys():\n",
    "            self.labels = kwargs['labels']\n",
    "        else:\n",
    "            self.labels = {'RuleBreak': {'WrongLane', 'WrongTurn', 'TrafficLight'}, 'LaneChanging': {'True'}, 'LaneChanging(m)': {\n",
    "                'True'}, 'OverTaking': {'True'}, 'Yield': {'True'}, 'Cutting': {'True'}, 'ZigzagMovement': {'True'}, 'OverSpeeding': {'True'}}\n",
    "        if 'labels_mapping' in kwargs.keys():\n",
    "            warnings.warn('if labels are mapped to the same value only the first one will be mentioned in info_json', category=UserWarning, stacklevel=2)\n",
    "            self.labels_mapping = kwargs['labels_mapping']\n",
    "        else:\n",
    "            self.labels_mapping = {'RuleBreak': 0, 'LaneChanging': 1, 'LaneChanging(m)': 1, 'OverTaking': 2, 'Yield': 3, 'Cutting': 4, 'ZigzagMovement': 5, 'OverSpeeding': 6}\n",
    "\n",
    "    def video_processor(self, video):\n",
    "        frames, _, _ = read_video(video, pts_unit='sec', output_format='TCHW')\n",
    "\n",
    "        rgb_frames = torch.stack([frames[i] for i in range(\n",
    "            frames.shape[0]) if i % self.frame_interval == 0])\n",
    "        rgb_frames = TF.resize(rgb_frames, size=self.resize)\n",
    "        # divide by 255 to map pixel values to interval [0,1]\n",
    "        rgb_frames = (rgb_frames/255).to(self.device)\n",
    "\n",
    "        # extract rgb_features\n",
    "        rgb_chunks = torch.split(rgb_frames, split_size_or_sections=self.batch_size, dim=0)\n",
    "        rgb_features = []\n",
    "\n",
    "        for chunk in rgb_chunks:\n",
    "            features = self.rgb_model(chunk)['flatten']\n",
    "            rgb_features.append(features)\n",
    "\n",
    "        # gc.collect(features, rgb_chunks, chunk)\n",
    "        rgb_features = torch.cat(rgb_features, dim=0).to('cpu')\n",
    "        rgb_features = rgb_features[1:]\n",
    "\n",
    "        # extract flow_features\n",
    "        flow_stack1, flow_stack2 = self.transforms(rgb_frames[:-1], rgb_frames[1:])\n",
    "        flow_stack1 = torch.split(flow_stack1, split_size_or_sections=self.batch_size, dim=0)\n",
    "        flow_stack2 = torch.split(flow_stack2, split_size_or_sections=self.batch_size, dim=0)\n",
    "\n",
    "        flow_features = []\n",
    "\n",
    "        for stack1, stack2 in zip(flow_stack1, flow_stack2):\n",
    "            features = self.flow_model(stack1, stack2)\n",
    "            # append last item from extracted flow_features, as it corresponds to last iteration (most accurate one)\n",
    "            features = flow_to_image(features[-1])\n",
    "            features = self.rgb_model(features/255)['flatten']\n",
    "            flow_features.append(features)\n",
    "\n",
    "        # gc.collect(features, stack1, stack2, flow_stack1, flow_stack2)\n",
    "        flow_features = torch.cat(flow_features, dim=0).to('cpu')\n",
    "\n",
    "        # assert flow_features.shape == rgb_features.shape, f'shapes of rgb ({rgb_features.shape}) and flow ({flow_features.shape}) features do not match'\n",
    "\n",
    "        return rgb_features.detach().cpu().resolve_conj().resolve_neg().numpy(), flow_features.detach().cpu().resolve_conj().resolve_neg().numpy()\n",
    "\n",
    "    # '/workspace/pvc-meteor/downloads/Video XML Annotations'\n",
    "    def annotation_processor(self, video_name):\n",
    "        zip_name = video_name[:-4] + '.zip'\n",
    "        # check if name exist\n",
    "\n",
    "        if zip_name not in os.listdir(self.anno_zip_dir):\n",
    "            self.error_file[video_name] = 'zip file not found'\n",
    "            return None\n",
    "\n",
    "\n",
    "        else:\n",
    "            # load zip object\n",
    "            zip_file = ZipFile(os.path.join(self.anno_zip_dir, zip_name))\n",
    "            nr_frames = len([i for i in zip_file.namelist() if '.xml' in i])\n",
    "            nr_frames = np.floor(nr_frames / self.frame_interval).astype(int)\n",
    "            template = np.zeros((nr_frames, 7))\n",
    "\n",
    "            append_folder = 'Annotations/' in zip_file.namelist()\n",
    "            # iterate through key-frames\n",
    "            for i_temp, i_frame in enumerate([i * self.frame_interval for i in range(nr_frames)]):\n",
    "                frame_name = 'frame_{0:06d}.xml'.format(i_frame)\n",
    "                frame_name = 'Annotations/' + frame_name if append_folder else f'{zip_name[:-4]}/Annotations/' + frame_name\n",
    "\n",
    "                xml_file = xmltodict.parse(zip_file.read(frame_name))['annotation']\n",
    "\n",
    "                if 'object' not in xml_file:\n",
    "                    # behave as if no file found\n",
    "                    return None\n",
    "\n",
    "                if not isinstance(xml_file['object'], list):\n",
    "                    xml_file['object'] = [xml_file['object']]\n",
    "\n",
    "                for obj in xml_file['object']:\n",
    "                    if obj['name'] == 'EgoVehicle':\n",
    "                        continue\n",
    "                    for attr in obj['attributes']['attribute']:\n",
    "                        if 'GPSData' in attr:\n",
    "                            continue\n",
    "                        if attr['name'] in self.labels:\n",
    "                            if attr['value'] in self.labels[attr['name']]:\n",
    "                                c_idx = self.labels_mapping[attr['name']]\n",
    "\n",
    "                                template[i_temp, c_idx] = 1\n",
    "        return template[1:]\n",
    "\n",
    "    def get_dict_for_info_json(self, output_dict, test_split=0.2):\n",
    "        \"\"\"generates dictionary for data_info_new.json as required by OadTR.\n",
    "\n",
    "        Args:\n",
    "            output_dict (dict, str): dictionary object contaning the key 'annotations' or filepath to such a dictionary.\n",
    "            test_split (float, optional): relative share of the data used for evaluation. Defaults to 0.2.\n",
    "\n",
    "        Returns:\n",
    "            dictionary: dictionary with the keys ['class_index', 'train_session_set', 'test_session_set']\n",
    "        \"\"\"\n",
    "        if isinstance(output_dict, str):\n",
    "            with open(output_dict, 'rb') as f:\n",
    "                output_dict = pickle.load(f)\n",
    "        \n",
    "        video_names = list(output_dict['annotations'].keys())\n",
    "        train, test = train_test_split(video_names, test_size=test_split)\n",
    "\n",
    "        seen_values = []\n",
    "        class_index = []\n",
    "        for k, v in self.labels_mapping.items():\n",
    "            if v in seen_values:\n",
    "                continue\n",
    "            else:\n",
    "                seen_values.append(v)\n",
    "                class_index.append(k)\n",
    "        \n",
    "        return {\n",
    "            'class_index': class_index,\n",
    "            'train_session_set': train,\n",
    "            'test_session_set': test\n",
    "            }\n",
    "\n",
    "\n",
    "    def file_to_features(self, file_path):\n",
    "        video_file_name = file_path[-29:]\n",
    "        zip_file_name = video_file_name[:-4] + '.zip'\n",
    "\n",
    "        annotation = self.annotation_processor(zip_file_name)\n",
    "\n",
    "        # skip video if no annotations are available\n",
    "        if not isinstance(annotation, np.ndarray):\n",
    "            return None, None\n",
    "\n",
    "        else:\n",
    "            rgb_features, flow_features = self.video_processor(\n",
    "                os.path.join(self.vid_dir, video_file_name))\n",
    "\n",
    "        if rgb_features.shape != flow_features.shape:\n",
    "            print(f'rgb_shape={rgb_features.shape}, flow_shape={flow_features.shape}')\n",
    "            self.error_file[video_file_name] = 'feature shape mismatch'\n",
    "            return None, None\n",
    "        if annotation.shape[0] != rgb_features.shape[0]:\n",
    "            self.error_file[video_file_name] = 'annotation shape mismatch'\n",
    "            return None, None\n",
    "\n",
    "        # create dictionary and write to pickle file\n",
    "        return {\n",
    "            video_file_name:\n",
    "                {\n",
    "                    'rgb': rgb_features,\n",
    "                    'flow': flow_features\n",
    "                }\n",
    "        }, {video_file_name: annotation}\n",
    "\n",
    "    def dir_to_features(self, save=True):\n",
    "        if use_mp: \n",
    "            warnings.warn('using multiprocessing with cuda is currently not supported', category=UserWarning, stacklevel=2)\n",
    "            \n",
    "        # keep start time to differentiate between pickled files\n",
    "        start_time = datetime.now()\n",
    "        start_time = start_time.strftime(\"%d-%m-%Y-%H-%M\")\n",
    "\n",
    "        general_informaion = {\n",
    "            'fps': self.frame_interval, \n",
    "            'rgb_extractor': self.rgb_model.__class__.__name__,\n",
    "            'flow_extractor': self.flow_model.__class__.__name__, \n",
    "            'extraction_time': start_time\n",
    "            }\n",
    "\n",
    "        output_dict = {'meta': general_informaion,\n",
    "                       'features': dict(), \n",
    "                       'annotations': dict()\n",
    "                       }\n",
    "\n",
    "\n",
    "        for file_path in tqdm(glob(self.vid_dir + '/*.MP4')):\n",
    "            try:\n",
    "                vid_features, vid_annot = self.file_to_features(file_path)\n",
    "\n",
    "                if vid_features == None:\n",
    "                    continue\n",
    "\n",
    "                output_dict['features'].update(vid_features)\n",
    "                output_dict['annotations'].update(vid_annot)\n",
    "\n",
    "\n",
    "                if save:\n",
    "\n",
    "                    file_name = f'file_features_{file_path[-29:-4]}.pkl'\n",
    "                    vid_save = {'meta': general_informaion,\n",
    "                        'features': vid_features, \n",
    "                        'annotations': vid_annot\n",
    "                        }\n",
    "\n",
    "                    with open(self.output_dir + file_name, 'wb') as file:\n",
    "                        pickle.dump(vid_save, file)\n",
    "\n",
    "            except:\n",
    "                self.error_file[file_path[-29:-4]] = 'catched by try-except block'\n",
    "                continue\n",
    "\n",
    "        output_dict['error_file'] = self.error_file\n",
    "        \n",
    "        if save:\n",
    "            # create the file name using the start_time and extracted_features variables\n",
    "            file_name = f'extracted_features_{start_time}.pkl'\n",
    "\n",
    "            # open the file in write mode\n",
    "            with open(self.output_dir + file_name, 'wb') as file:\n",
    "                # use pickle to serialize the dictionary and write it to the file\n",
    "                pickle.dump(output_dict, file)\n",
    "\n",
    "            json_dict = self.get_dict_for_info_json(output_dict)\n",
    "            with open(self.output_dir + 'data_info_new.json', 'w') as file:\n",
    "                json.dump(json_dict, file)\n",
    "        \n",
    "        return output_dict, json_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prototype = FeatureExtractor(\n",
    "    # METEOR Dataset has 30 FPS --> only taking every 30th frame means using only 1 frame every second.\n",
    "    FPS=15,\n",
    "    vid_dir='../data_sample/videos/',\n",
    "    anno_zip_dir='../data_sample/annotations/'\n",
    "    output_dir='../data_sample/output/',\n",
    ")\n",
    "\n",
    "# prototype.video_processor('/workspace/persistent/data_sample/videos/REC_1970_01_01_07_40_16_F.MP4')\n",
    "# prototype.annotation_processor2('REC_1970_01_01_07_40_16_F.zip', '/workspace/persistent/data sample/annotations/frame')\n",
    "# prototype.dir_to_features()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.16 ('thesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "21e8d3179073958173d6cf46a62335ced4bd9060b16a08c7dd2ce09d931d915c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
