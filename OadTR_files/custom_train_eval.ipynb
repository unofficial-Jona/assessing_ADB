{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7399c066-76da-4396-8b47-be74530191a5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Experimenting with OadTR\n",
    "\n",
    "this notebook is dedicated to conduct training and evaluation of the OadTR model using the METEOR dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d073e-9afb-4517-9419-ab54ddaf0020",
   "metadata": {
    "tags": []
   },
   "source": [
    "## imports and device specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98fd1f42-4cbf-45ee-8773-041e81da7229",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'util'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutil\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mutl\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'util'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import util as utl\n",
    "import os\n",
    "import utils\n",
    "import datetime\n",
    "import argparse\n",
    "import random\n",
    "\n",
    "import transformer_models\n",
    "from custom_dataset import METEORDataLayer\n",
    "from train import train_one_epoch, evaluate\n",
    "from test import test_one_epoch\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "from torchsummary import summary \n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'selected device is {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223856b3-dd7c-4ec0-ae13-a69dae95a340",
   "metadata": {
    "tags": []
   },
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ea29b-fe52-4f0a-bd8b-b2697b305550",
   "metadata": {},
   "source": [
    "### set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d48061d-7c0c-4920-b0e8-3816dab09426",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up args --> parser will collapse to default values\n",
    "\n",
    "def str2bool(string):\n",
    "    return True if string.lower() == 'true' else False\n",
    "\n",
    "\n",
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('Set IDU Online Detector', add_help=False)\n",
    "    parser.add_argument('--lr', default=1e-4, type=float)     # 1e-4\n",
    "    parser.add_argument('--batch_size', default=128, type=int)\n",
    "    parser.add_argument('--weight_decay', default=1e-4, type=float)\n",
    "    parser.add_argument('--epochs', default=5, type=int)\n",
    "    parser.add_argument('--resize_feature', default=False, type=str2bool, help='run resize prepare_data or not')\n",
    "    parser.add_argument('--lr_drop', default=1, type=int)\n",
    "    parser.add_argument('--clip_max_norm', default=1., type=float,\n",
    "                        help='gradient clipping max norm')  # dataparallel\n",
    "    parser.add_argument('--dataparallel', action='store_true', help='multi-gpus for training')\n",
    "    parser.add_argument('--removelog', action='store_true', help='remove old log')\n",
    "\n",
    "    # * Network\n",
    "    parser.add_argument('--version', default='v3', type=str,\n",
    "                        help=\"fixed or learned\")  # learned  fixed\n",
    "    # decoder\n",
    "    parser.add_argument('--query_num', default=8, type=int,\n",
    "                        help=\"Number of query_num (prediction)\")\n",
    "    parser.add_argument('--decoder_layers', default=5, type=int,\n",
    "                        help=\"Number of decoder_layers\")\n",
    "    parser.add_argument('--decoder_embedding_dim', default=1024, type=int,   # 1024\n",
    "                        help=\"decoder_embedding_dim\")\n",
    "    parser.add_argument('--decoder_embedding_dim_out', default=1024, type=int,  # 256 512 1024\n",
    "                        help=\"decoder_embedding_dim_out\")\n",
    "    parser.add_argument('--decoder_attn_dropout_rate', default=0.1, type=float,  # 0.1=0.2\n",
    "                        help=\"rate of decoder_attn_dropout_rate\")\n",
    "    parser.add_argument('--decoder_num_heads', default=4, type=int,  # 8 4\n",
    "                        help=\"decoder_num_heads\")\n",
    "    parser.add_argument('--classification_pred_loss_coef', default=0.5, type=float)  # 0.5\n",
    "\n",
    "    # encoder\n",
    "    parser.add_argument('--enc_layers', default=64, type=int,\n",
    "                        help=\"Number of enc_layers\")\n",
    "    parser.add_argument('--lr_backbone', default=1e-4, type=float,    # 2e-4\n",
    "                        help=\"lr_backbone\")\n",
    "    parser.add_argument('--feature', default='ResNet50', type=str,\n",
    "                        help=\"feature type\")\n",
    "    parser.add_argument('--dim_feature', default=2048, type=int,\n",
    "                        help=\"input feature dims\")\n",
    "    parser.add_argument('--patch_dim', default=1, type=int,\n",
    "                        help=\"input feature dims\")\n",
    "    parser.add_argument('--embedding_dim', default=1024, type=int,  # 1024\n",
    "                        help=\"input feature dims\")\n",
    "    parser.add_argument('--num_heads', default=4, type=int,\n",
    "                        help=\"input feature dims\")\n",
    "    parser.add_argument('--num_layers', default=3, type=int,\n",
    "                        help=\"input feature dims\")\n",
    "    parser.add_argument('--attn_dropout_rate', default=0.1, type=float,\n",
    "                        help=\"attn dropout\")\n",
    "    parser.add_argument('--positional_encoding_type', default='learned', type=str,\n",
    "                        help=\"fixed or learned\")  # learned  fixed\n",
    "\n",
    "    parser.add_argument('--hidden_dim', default=1024, type=int,  # 512 1024\n",
    "                        help=\"Size of the embeddings\")\n",
    "    parser.add_argument('--dropout_rate', default=0.1, type=float,\n",
    "                        help=\"Dropout applied \")\n",
    "\n",
    "    parser.add_argument('--numclass', default=7, type=int,\n",
    "                        help=\"Number of class\")\n",
    "\n",
    "    # * Loss coefficients\n",
    "    parser.add_argument('--classification_x_loss_coef', default=0.3, type=float)\n",
    "    parser.add_argument('--classification_h_loss_coef', default=1, type=float)\n",
    "    parser.add_argument('--similar_loss_coef', default=0.1, type=float)   # 0.3\n",
    "    parser.add_argument('--margin', default=1., type=float)\n",
    "\n",
    "    # dataset parameters\n",
    "    parser.add_argument('--dataset_file', type=str, default='../../pvc-meteor/features/data_info_new.json')\n",
    "    parser.add_argument('--frozen_weights', type=str, default=None)\n",
    "    '''\n",
    "    parser.add_argument('--thumos_data_path', type=str, default='/home/dancer/mycode/Temporal.Online.Detection/'\n",
    "                                                                'Online.TRN.Pytorch/preprocess/')\n",
    "    parser.add_argument('--thumos_anno_path', type=str, default='data/thumos_{}_anno.pickle')\n",
    "    '''\n",
    "    parser.add_argument('--remove_difficult', action='store_true')\n",
    "    parser.add_argument('--device', default=device,\n",
    "                        help='device to use for training / testing')\n",
    "\n",
    "    parser.add_argument('--output_dir', default='models',\n",
    "                        help='path where to save, empty for no saving')\n",
    "    parser.add_argument('--seed', default=20, type=int)\n",
    "    parser.add_argument('--resume', default='', help='resume from checkpoint')\n",
    "    parser.add_argument('--start_epoch', default=1, type=int, metavar='N',\n",
    "                        help='start epoch')\n",
    "\n",
    "    parser.add_argument('--eval', action='store_true')\n",
    "    parser.add_argument('--num_workers', default=8, type=int)\n",
    "\n",
    "    # distributed training parameters\n",
    "    parser.add_argument('--world_size', default=1, type=int,\n",
    "                        help='number of distributed processes')\n",
    "    parser.add_argument('--dist_url', default='tcp://127.0.0.1:12342', help='url used to set up distributed training')\n",
    "    # 'env://'\n",
    "    return parser\n",
    "\n",
    "args = get_args_parser().parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632833a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up model according to args\n",
    "model = transformer_models.VisionTransformer_v3(args=args, img_dim=args.enc_layers,   # VisionTransformer_v3\n",
    "                                                patch_dim=args.patch_dim,\n",
    "                                                out_dim=args.numclass,\n",
    "                                                embedding_dim=args.embedding_dim,\n",
    "                                                num_heads=args.num_heads,\n",
    "                                                num_layers=args.num_layers,\n",
    "                                                hidden_dim=args.hidden_dim,\n",
    "                                                dropout_rate=args.dropout_rate,\n",
    "                                                attn_dropout_rate=args.attn_dropout_rate,\n",
    "                                                num_channels=args.dim_feature,\n",
    "                                                positional_encoding_type=args.positional_encoding_type\n",
    "                                                )\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "summary(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c73eeac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define losses for encoder/decoder\n",
    "loss_need = [\n",
    "    'labels_encoder',\n",
    "    'labels_decoder',\n",
    "]\n",
    "criterion = utl.SetCriterion(num_classes=args.numclass, losses=loss_need, args=args).to(device)\n",
    "\n",
    "# define optiimizer and lr_scheduler\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay,)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.lr_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load training data\n",
    "dataset_train = METEORDataLayer(phase='train', args=args)\n",
    "\n",
    "# define train sampler\n",
    "sampler_train = torch.utils.data.RandomSampler(dataset_train)\n",
    "\n",
    "# define data loader\n",
    "data_loader_train = DataLoader(dataset_train, batch_sampler=batch_sampler_train, pin_memory=True, num_workers=args.num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d569c5a",
   "metadata": {},
   "source": [
    "### training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d552b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logger\n",
    "logger = utl.setup_logger(os.path.join(this_dir, 'log_dist.txt'), command=command)\n",
    "\n",
    "# save args\n",
    "for arg in vars(args):\n",
    "    logger.output_print(\"{}:{}\".format(arg, getattr(args, arg)))\n",
    "\n",
    "# trainable parameters in model\n",
    "n_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "logger.output_print('number of params: {}'.format(n_parameters))\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    if args.distributed:\n",
    "        sampler_train.set_epoch(epoch)\n",
    "    train_stats = train_one_epoch(\n",
    "        model, criterion, data_loader_train, optimizer, device, epoch,\n",
    "        args.clip_max_norm)\n",
    "\n",
    "    lr_scheduler.step()\n",
    "    if args.output_dir:\n",
    "        checkpoint_paths = [output_dir / 'checkpoint.pth']\n",
    "        # extra checkpoint before LR drop and every 100 epochs\n",
    "        if (epoch + 1) % args.lr_drop == 0 or (epoch + 1) % 100 == 0:\n",
    "            checkpoint_paths.append(output_dir / f'checkpoint{epoch:04}.pth')\n",
    "        for checkpoint_path in checkpoint_paths:\n",
    "            utils.save_on_master({\n",
    "                'model': model_without_ddp.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'args': args,\n",
    "            }, checkpoint_path)\n",
    "\n",
    "    test_stats = evaluate(\n",
    "        model, criterion, data_loader_val, device, logger, args, epoch, nprocs=utils.get_world_size()\n",
    "    )\n",
    "\n",
    "    log_stats = {**{f'train_{k}': v for k, v in train_stats.items()},\n",
    "                    **{f'test_{k}': v for k, v in test_stats.items()},\n",
    "                    'epoch': epoch,\n",
    "                    'n_parameters': n_parameters}\n",
    "\n",
    "    if args.output_dir and utils.is_main_process():\n",
    "        with (output_dir / \"log_trai n&test.txt\").open(\"a\") as f:\n",
    "            f.write(json.dumps(log_stats) + \"\\n\")\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print('Training time {}'.format(total_time_str))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a510aba7-ec1d-4ca2-a1f7-f3c3593c1d94",
   "metadata": {
    "tags": []
   },
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6463c52a-eac9-4cc7-9cc4-22db6d3c4614",
   "metadata": {
    "tags": []
   },
   "source": [
    "### load model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
